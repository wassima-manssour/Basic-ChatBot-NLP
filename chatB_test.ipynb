{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPvniZ/U37iQerSdWOiIrRl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wassima-manssour/Basic-ChatBot-NLP/blob/main/chatB_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Workflow of ChatBot**"
      ],
      "metadata": {
        "id": "dvOJ2XS0HQXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Determine the objective of the chatbot and set goals\n",
        "2.   Create a conversational architecture\n",
        "3. Collection of question variation\n",
        "4. Select the development approach\n",
        "5. Implementing the dialogie flow (NLP)\n",
        "6. Testing and revising the use case\n",
        "\n"
      ],
      "metadata": {
        "id": "_CqHRWTaHWgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import Dependencies**"
      ],
      "metadata": {
        "id": "Q5waYYcFPpel"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0RoL9zcMtvKp"
      },
      "outputs": [],
      "source": [
        "#!pip install tflearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tflearn \n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "from time import sleep"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y83Mgq32uRek",
        "outputId": "3df2237b-58a3-4197-bd73-53c2c873e06d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NwmlsGogQMCt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create needed functions**"
      ],
      "metadata": {
        "id": "CmoeFRn0P1f_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(sentence):\n",
        "  sntnce_nopunt = [w for w in sentence if w not in string.punctuation ]\n",
        "  return sntnce_nopunt"
      ],
      "metadata": {
        "id": "8YQF5T_kYCaq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TRSqmzI-GF5o"
      },
      "outputs": [],
      "source": [
        "#from nltk.tokenize import word_tokenize\n",
        "\n",
        "def tokenize(sentence):\n",
        "    \"\"\"\n",
        "    split sentence into array of words/tokens\n",
        "    a token can be a word or punctuation character, or number\n",
        "    \"\"\"\n",
        "    return nltk.word_tokenize(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stem(word):\n",
        "    \"\"\"\n",
        "    stemming = find the root form of the word\n",
        "    examples:\n",
        "    words = [\"organize\", \"organizes\", \"organizing\"]\n",
        "    words = [stem(w) for w in words]\n",
        "    -> [\"organ\", \"organ\", \"organ\"]\n",
        "    \"\"\"\n",
        "    return stemmer.stem(word.lower())\n"
      ],
      "metadata": {
        "id": "qlURv6HfQKan"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(s, words):\n",
        "    \"\"\"\n",
        "    return bag of words array:\n",
        "    1 for each known word that exists in the sentence, 0 otherwise\n",
        "    example:\n",
        "    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
        "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
        "    bog   = [  0 ,    1 ,    0 ,   1 ,    0 ,    0 ,      0]\n",
        "    \"\"\"\n",
        "    bag = [0 for _ in range(len(words))]\n",
        "    s_words = nltk.word_tokenize(s)\n",
        "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
        "\n",
        "    for s_word in s_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == s_word:\n",
        "                bag[i] = 1\n",
        "\n",
        "    return np.array(bag)"
      ],
      "metadata": {
        "id": "HKz6oIEKLjtT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "lemmenization\n",
        " import enchant\n",
        " d = enchant.Dict(\"en_US\")\n",
        " d.check(\"Hello\")\n",
        " d.check(\"Helo\")\n",
        " d.suggest(\"Helo\")\n",
        "'''"
      ],
      "metadata": {
        "id": "hwDIq-OmVLFm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3a9c208d-82f1-49d6-f65d-5318f3dc8154"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nlemmenization\\n import enchant\\n d = enchant.Dict(\"en_US\")\\n d.check(\"Hello\")\\n d.check(\"Helo\")\\n d.suggest(\"Helo\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uk2T6jQxQScL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Data**"
      ],
      "metadata": {
        "id": "7__CZiRFEprB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/intents.json','r') as f:\n",
        "  data = json.load(f)"
      ],
      "metadata": {
        "id": "2a3TYJNKQOa4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s8A2307NOuF",
        "outputId": "c6893dd5-0c78-487f-910b-428a81ca1cdd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'intents': [{'tag': 'greeting', 'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day', 'Whats up', 'Good morning', 'Good evening', 'hello', 'hey', \"what's up\"], 'responses': ['Hello!', 'Good to see you!', 'Hi there, how can I help?'], 'context_set': ''}, {'tag': 'goodbye', 'patterns': ['cya', 'See you later', 'Goodbye', 'I am Leaving', 'Have a Good day', 'bye', 'i have to go', 'gotta go'], 'responses': ['Sad to see you go :(', 'Talk to you later', 'Goodbye!'], 'context_set': ''}, {'tag': 'age', 'patterns': ['how old', 'how old are you', 'what is your age', 'how old are you', 'age?'], 'responses': ['I am old and wise', 'trying to stay fit and young!'], 'context_set': ''}, {'tag': 'name', 'patterns': ['what is your name', 'what should I call you', 'whats your name?', 'who are you?'], 'responses': ['You can call me Sally.', \"I'm Sally!\", \"I'm Sally.\"], 'context_set': ''}, {'tag': 'help', 'patterns': ['Id like to ask something', 'what can you do', 'can you help me?', 'can i tell you something'], 'responses': [\"I'm here to help you!\", 'I can help you with whatever you tell me to.'], 'context_set': ''}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Tokenize**"
      ],
      "metadata": {
        "id": "-zk8tShDQYr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting informations from intents.json--\n",
        "words = [] # words to be tokenized\n",
        "labels = [] # tags of the intents data\n",
        "x_docs = [] # initial words before processinf(tokenization, steming...)\n",
        "y_docs = [] # ['word','tag']  \n",
        "\n",
        "for intent in data['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        wrds = tokenize(pattern)\n",
        "        words.extend(wrds)\n",
        "        x_docs.append(wrds)\n",
        "        y_docs.append(intent['tag'])\n",
        "\n",
        "        if intent['tag'] not in labels:\n",
        "            labels.append(intent['tag'])"
      ],
      "metadata": {
        "id": "7yTCQzDIQfwp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'words: {words}\\ntags: {labels} \\ninitail words: {x_docs} \\nword-tag: {y_docs}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vFcY-nXQtuB",
        "outputId": "65d39b51-8a61-4bc0-8b60-db5ade961045"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words: ['Hi', 'How', 'are', 'you', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Whats', 'up', 'Good', 'morning', 'Good', 'evening', 'hello', 'hey', 'what', \"'s\", 'up', 'cya', 'See', 'you', 'later', 'Goodbye', 'I', 'am', 'Leaving', 'Have', 'a', 'Good', 'day', 'bye', 'i', 'have', 'to', 'go', 'got', 'ta', 'go', 'how', 'old', 'how', 'old', 'are', 'you', 'what', 'is', 'your', 'age', 'how', 'old', 'are', 'you', 'age', '?', 'what', 'is', 'your', 'name', 'what', 'should', 'I', 'call', 'you', 'whats', 'your', 'name', '?', 'who', 'are', 'you', '?', 'Id', 'like', 'to', 'ask', 'something', 'what', 'can', 'you', 'do', 'can', 'you', 'help', 'me', '?', 'can', 'i', 'tell', 'you', 'something']\n",
            "tags: ['greeting', 'goodbye', 'age', 'name', 'help'] \n",
            "initail words: [['Hi'], ['How', 'are', 'you'], ['Is', 'anyone', 'there', '?'], ['Hello'], ['Good', 'day'], ['Whats', 'up'], ['Good', 'morning'], ['Good', 'evening'], ['hello'], ['hey'], ['what', \"'s\", 'up'], ['cya'], ['See', 'you', 'later'], ['Goodbye'], ['I', 'am', 'Leaving'], ['Have', 'a', 'Good', 'day'], ['bye'], ['i', 'have', 'to', 'go'], ['got', 'ta', 'go'], ['how', 'old'], ['how', 'old', 'are', 'you'], ['what', 'is', 'your', 'age'], ['how', 'old', 'are', 'you'], ['age', '?'], ['what', 'is', 'your', 'name'], ['what', 'should', 'I', 'call', 'you'], ['whats', 'your', 'name', '?'], ['who', 'are', 'you', '?'], ['Id', 'like', 'to', 'ask', 'something'], ['what', 'can', 'you', 'do'], ['can', 'you', 'help', 'me', '?'], ['can', 'i', 'tell', 'you', 'something']] \n",
            "word-tag: ['greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'age', 'age', 'age', 'age', 'age', 'name', 'name', 'name', 'name', 'help', 'help', 'help', 'help']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.Remove punctuation**"
      ],
      "metadata": {
        "id": "1goC2PJYZUJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=remove_punctuation(words)"
      ],
      "metadata": {
        "id": "SaEnZgXNQ3-D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsjuEderQ3yx",
        "outputId": "b07b7166-2a37-47e8-84b9-b6c5b48fa85b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi', 'How', 'are', 'you', 'Is', 'anyone', 'there', 'Hello', 'Good', 'day', 'Whats', 'up', 'Good', 'morning', 'Good', 'evening', 'hello', 'hey', 'what', \"'s\", 'up', 'cya', 'See', 'you', 'later', 'Goodbye', 'I', 'am', 'Leaving', 'Have', 'a', 'Good', 'day', 'bye', 'i', 'have', 'to', 'go', 'got', 'ta', 'go', 'how', 'old', 'how', 'old', 'are', 'you', 'what', 'is', 'your', 'age', 'how', 'old', 'are', 'you', 'age', 'what', 'is', 'your', 'name', 'what', 'should', 'I', 'call', 'you', 'whats', 'your', 'name', 'who', 'are', 'you', 'Id', 'like', 'to', 'ask', 'something', 'what', 'can', 'you', 'do', 'can', 'you', 'help', 'me', 'can', 'i', 'tell', 'you', 'something']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Steming & removing stop words**"
      ],
      "metadata": {
        "id": "W5QXQ7fhQebz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download ('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "#print(stop_words)\n",
        "words=(stem(word) for word in words if word not in stop_words) # lower & stem words\n",
        "words = sorted(set(words)) # get just unique words\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRs_hi8RQ6Xc",
        "outputId": "92161fda-875e-4efd-c20e-86f6c580fe48"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'s\", 'ag', 'anyon', 'ask', 'bye', 'cal', 'cya', 'day', 'ev', 'go', 'good', 'goodby', 'got', 'hav', 'hello', 'help', 'hey', 'hi', 'id', 'lat', 'leav', 'lik', 'morn', 'nam', 'old', 'see', 'ta', 'tel']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_docs), \"patterns\",x_docs)\n",
        "print(len(labels), \"tags:\", labels)\n",
        "print(len(words), \"unique stemmed words:\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHcF8L5_RWJb",
        "outputId": "b1680e50-4bbb-4c6f-e84d-1f2c7aa6fd35"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32 patterns [['Hi'], ['How', 'are', 'you'], ['Is', 'anyone', 'there', '?'], ['Hello'], ['Good', 'day'], ['Whats', 'up'], ['Good', 'morning'], ['Good', 'evening'], ['hello'], ['hey'], ['what', \"'s\", 'up'], ['cya'], ['See', 'you', 'later'], ['Goodbye'], ['I', 'am', 'Leaving'], ['Have', 'a', 'Good', 'day'], ['bye'], ['i', 'have', 'to', 'go'], ['got', 'ta', 'go'], ['how', 'old'], ['how', 'old', 'are', 'you'], ['what', 'is', 'your', 'age'], ['how', 'old', 'are', 'you'], ['age', '?'], ['what', 'is', 'your', 'name'], ['what', 'should', 'I', 'call', 'you'], ['whats', 'your', 'name', '?'], ['who', 'are', 'you', '?'], ['Id', 'like', 'to', 'ask', 'something'], ['what', 'can', 'you', 'do'], ['can', 'you', 'help', 'me', '?'], ['can', 'i', 'tell', 'you', 'something']]\n",
            "5 tags: ['greeting', 'goodbye', 'age', 'name', 'help']\n",
            "28 unique stemmed words: [\"'s\", 'ag', 'anyon', 'ask', 'bye', 'cal', 'cya', 'day', 'ev', 'go', 'good', 'goodby', 'got', 'hav', 'hello', 'help', 'hey', 'hi', 'id', 'lat', 'leav', 'lik', 'morn', 'nam', 'old', 'see', 'ta', 'tel']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Building the model**"
      ],
      "metadata": {
        "id": "ufw0RMOFR7Er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training = []\n",
        "output = []\n",
        "out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "# One hot encoding, Converting the words to numerals\n",
        "for x, doc in enumerate(x_docs):\n",
        "    bag = []\n",
        "    wrds = [stemmer.stem(w) for w in doc]\n",
        "    for w in words:\n",
        "        if w in wrds:\n",
        "            bag.append(1)\n",
        "        else:\n",
        "            bag.append(0)\n",
        "\n",
        "\n",
        "    output_row = out_empty[:]\n",
        "    output_row[labels.index(y_docs[x])] = 1\n",
        "\n",
        "    training.append(bag)\n",
        "    output.append(output_row)\n",
        "\n",
        "\n",
        "training = np.array(training)\n",
        "output = np.array(output)"
      ],
      "metadata": {
        "id": "xzbenGeJRzeU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = tflearn.input_data(shape=[None, len(training[0])])\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, len(output[0]), activation='softmax')\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "model = tflearn.DNN(net)\n",
        "model.fit(training, output, n_epoch=500, batch_size=8, show_metric=True)\n",
        "model.save('model.tflearn')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwjUx0P6SCTF",
        "outputId": "23b4cbfc-5767-4701-f919-a12266d3f5b2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 1999  | total loss: \u001b[1m\u001b[32m0.40273\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 500 | loss: 0.40273 - acc: 0.8858 -- iter: 24/32\n",
            "Training Step: 2000  | total loss: \u001b[1m\u001b[32m0.42368\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 500 | loss: 0.42368 - acc: 0.8722 -- iter: 32/32\n",
            "--\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "\n",
        "    while True:\n",
        "        inp = input(\"\\n\\nYou: \")\n",
        "        if inp.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "    #Porbability of correct response \n",
        "        results = model.predict([bag_of_words(inp, words)])\n",
        "\n",
        "    # Picking the greatest number from probability\n",
        "        results_index = np.argmax(results)\n",
        "\n",
        "        tag = labels[results_index]\n",
        "\n",
        "        for tg in data['intents']:\n",
        "\n",
        "            if tg['tag'] == tag:\n",
        "                responses = tg['responses']\n",
        "                print(\"Bot:\\t\" + random.choice(responses))"
      ],
      "metadata": {
        "id": "9DG-0bpmSGHB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmGDi_x-SJ7g",
        "outputId": "3593262f-f452-4457-fe23-a1b4c818440a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "You: hey\n",
            "Bot:\tHi there, how can I help?\n",
            "\n",
            "\n",
            "You: your name?\n",
            "Bot:\tI'm Sally!\n",
            "\n",
            "\n",
            "You: ur age?\n",
            "Bot:\tI am old and wise\n",
            "\n",
            "\n",
            "You: what can you do\n",
            "Bot:\tSad to see you go :(\n",
            "\n",
            "\n",
            "You: what can u do?\n",
            "Bot:\tSad to see you go :(\n",
            "\n",
            "\n",
            "You: can you help me?\n",
            "Bot:\tI can help you with whatever you tell me to.\n",
            "\n",
            "\n",
            "You: ok\n",
            "Bot:\tGoodbye!\n",
            "\n",
            "\n",
            "You: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9HjBc2aQTViF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}